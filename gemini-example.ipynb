{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cbe38bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1240fdab",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------\n",
    "# 1. DATA LOADING AND CLEANING\n",
    "# ---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2a10f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_data(csv_path):\n",
    "    print(\">>> Loading Data...\")\n",
    "    # Loading the data (assuming standard CSV format based on your snippet)\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Selecting only necessary columns\n",
    "    df = df[['CUST_ORDER_ID', 'PART_ID', 'LOCATION_RANK']]\n",
    "    \n",
    "    # Sort by Order ID and then by Rank to ensure the sequence is correct\n",
    "    df = df.sort_values(by=['CUST_ORDER_ID', 'LOCATION_RANK'])\n",
    "    \n",
    "    # Convert PART_ID to string to ensure consistency\n",
    "    df['PART_ID'] = df['PART_ID'].astype(str)\n",
    "    \n",
    "    print(f\"Data loaded: {len(df)} rows.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb97caee",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------\n",
    "# 2. DATA PREPROCESSING (Tokenization & Sequence Creation)\n",
    "# ---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d507a293",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequencePreprocessor:\n",
    "    def __init__(self):\n",
    "        self.le = LabelEncoder()\n",
    "        self.vocab_size = 0\n",
    "        self.max_sequence_len = 0\n",
    "\n",
    "    def fit_transform(self, df):\n",
    "        print(\">>> Preprocessing sequences...\")\n",
    "        \n",
    "        # Encode Part IDs to Integers\n",
    "        df['PART_ID_ENCODED'] = self.le.fit_transform(df['PART_ID'])\n",
    "        self.vocab_size = len(self.le.classes_) + 1 # +1 for padding index 0\n",
    "        \n",
    "        # Group by Order ID to get lists of parts\n",
    "        order_groups = df.groupby('CUST_ORDER_ID')['PART_ID_ENCODED'].apply(list)\n",
    "        \n",
    "        # Generate N-gram sequences\n",
    "        # If an order is [A, B, C], we create inputs: [A] -> predict B, [A, B] -> predict C\n",
    "        input_sequences = []\n",
    "        for sequence in order_groups:\n",
    "            for i in range(1, len(sequence)):\n",
    "                n_gram_sequence = sequence[:i+1]\n",
    "                input_sequences.append(n_gram_sequence)\n",
    "        \n",
    "        # Pad sequences to ensure uniform length\n",
    "        self.max_sequence_len = max([len(x) for x in input_sequences])\n",
    "        input_sequences = np.array(pad_sequences(input_sequences, maxlen=self.max_sequence_len, padding='pre'))\n",
    "        \n",
    "        # Split into X (Features) and y (Target)\n",
    "        X, y = input_sequences[:, :-1], input_sequences[:, -1]\n",
    "        \n",
    "        # One-hot encode the output (y) is typically too memory intensive for large vocabularies.\n",
    "        # We will use Sparse Categorical Crossentropy loss, so we keep y as integers.\n",
    "        \n",
    "        print(f\"Vocabulary Size: {self.vocab_size}\")\n",
    "        print(f\"Max Sequence Length: {self.max_sequence_len}\")\n",
    "        print(f\"Training Samples: {len(X)}\")\n",
    "        \n",
    "        return X, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c7de1e",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------\n",
    "# 3. MODEL CREATION (LSTM)\n",
    "# ---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12fc1888",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_model(vocab_size, max_seq_len, embedding_dim=64):\n",
    "    print(\">>> Building Model...\")\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Embedding Layer: Turns integer Part IDs into dense vectors\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_seq_len-1))\n",
    "    \n",
    "    # LSTM Layer: Captures the sequence order dependencies\n",
    "    model.add(LSTM(100, return_sequences=False))\n",
    "    model.add(Dropout(0.2)) # Prevent overfitting\n",
    "    \n",
    "    # Output Layer: Probability distribution over all possible parts\n",
    "    model.add(Dense(vocab_size, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45009fb",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------\n",
    "# 4. IMPLEMENTATION & PREDICTION\n",
    "# ---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2098909",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_part(model, preprocessor, current_sequence_of_parts):\n",
    "    # 1. Encode the input list of parts\n",
    "    encoded_seq = []\n",
    "    for part in current_sequence_of_parts:\n",
    "        try:\n",
    "            encoded_seq.append(preprocessor.le.transform([str(part)])[0])\n",
    "        except ValueError:\n",
    "            # Handle unknown parts (parts not seen in training)\n",
    "            continue\n",
    "            \n",
    "    if not encoded_seq:\n",
    "        return \"Unknown Sequence\"\n",
    "\n",
    "    # 2. Pad the sequence\n",
    "    padded_seq = pad_sequences([encoded_seq], maxlen=preprocessor.max_sequence_len-1, padding='pre')\n",
    "    \n",
    "    # 3. Predict\n",
    "    predicted_probs = model.predict(padded_seq, verbose=0)\n",
    "    predicted_class = np.argmax(predicted_probs, axis=-1)[0]\n",
    "    \n",
    "    # 4. Decode back to Part ID\n",
    "    predicted_part = preprocessor.le.inverse_transform([predicted_class])[0]\n",
    "    \n",
    "    return predicted_part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fbc88c",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------\n",
    "# MAIN EXECUTION BLOCK\n",
    "# ---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9803fe40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Loading Data...\n",
      "Data loaded: 4205 rows.\n",
      ">>> Preprocessing sequences...\n",
      "Vocabulary Size: 337\n",
      "Max Sequence Length: 32\n",
      "Training Samples: 3428\n",
      ">>> Building Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "x:\\Projects\\Machine Learning\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      ">>> Training Model...\n",
      "Epoch 1/10\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.0333 - loss: 5.2059\n",
      "Epoch 2/10\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.0370 - loss: 4.9553\n",
      "Epoch 3/10\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.0385 - loss: 4.8792\n",
      "Epoch 4/10\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.0508 - loss: 4.7865\n",
      "Epoch 5/10\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.0668 - loss: 4.6800\n",
      "Epoch 6/10\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.0785 - loss: 4.5919\n",
      "Epoch 7/10\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.0849 - loss: 4.5092\n",
      "Epoch 8/10\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.0913 - loss: 4.4312\n",
      "Epoch 9/10\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.1009 - loss: 4.3690\n",
      "Epoch 10/10\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.1047 - loss: 4.3124\n",
      ">>> Training Complete.\n",
      "\n",
      ">>> Testing Prediction:\n",
      "Input Sequence: ['1030076001', '1050065051']\n",
      "Predicted Next Part: 1081084001\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # A. Configuration\n",
    "    FILE_NAME = 'newTestingData.csv'\n",
    "    EPOCHS = 10  # Increase this for better accuracy\n",
    "    BATCH_SIZE = 32\n",
    "\n",
    "    # B. Run Pipeline\n",
    "    try:\n",
    "        # Load\n",
    "        df = load_and_clean_data(FILE_NAME)\n",
    "        \n",
    "        # Preprocess\n",
    "        processor = SequencePreprocessor()\n",
    "        X, y = processor.fit_transform(df)\n",
    "        \n",
    "        # Model\n",
    "        model = create_lstm_model(processor.vocab_size, processor.max_sequence_len)\n",
    "        print(model.summary())\n",
    "        \n",
    "        # Train\n",
    "        print(\">>> Training Model...\")\n",
    "        model.fit(X, y, epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=1)\n",
    "        print(\">>> Training Complete.\")\n",
    "        \n",
    "        # C. Demonstration / Testing\n",
    "        print(\"\\n>>> Testing Prediction:\")\n",
    "        \n",
    "        # Let's pick a real order from the data to test: CUST_ORDER_ID '033180'\n",
    "        # In the data, order 033180 goes: D5345600 -> F6546300 -> E5465900...\n",
    "        \n",
    "        test_sequence = ['1030076001', '1050065051'] # Actual Part IDs from the file for order 033180\n",
    "        print(f\"Input Sequence: {test_sequence}\")\n",
    "        \n",
    "        prediction = predict_next_part(model, processor, test_sequence)\n",
    "        print(f\"Predicted Next Part: {prediction}\")\n",
    "        \n",
    "        # In the file, the part after those two is '1083312001' (E5465900)\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Make sure '{FILE_NAME}' is in the same directory.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
